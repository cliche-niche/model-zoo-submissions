{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RepVGG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPJH5+NCvSLFmNnIixYxXpJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cliche-niche/model-zoo-submissions/blob/main/RepVGG/RepVGG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgJw8Ur4FyRt"
      },
      "source": [
        "An implementation of [RepVGG](https://arxiv.org/pdf/2101.03697.pdf), A0 architecture. It tests upon CIFAR-10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_0Z-KOiQQiZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8GJ6Uu5GPvM"
      },
      "source": [
        "Loading the data, subtracting the mean, and applying horizontal flip augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKKmidni4-1P"
      },
      "source": [
        "(trainIm, trainLab), (testIm, testLab) = tf.keras.datasets.cifar10.load_data()\n",
        "trainIm = trainIm / 255.0\n",
        "testIm = testIm / 255.0\n",
        "\n",
        "trainImMean = np.mean(trainIm, axis=0)\n",
        "trainIm -= trainImMean\n",
        "testIm -= trainImMean\n",
        "\n",
        "trainLab = tf.keras.utils.to_categorical(trainLab, 10)\n",
        "testLab = tf.keras.utils.to_categorical(testLab, 10)\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True)\n",
        "datagen.fit(trainIm)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enp3_YqzGam9"
      },
      "source": [
        "Code of a single stage.\n",
        "\n",
        "The first layer of a stage down-samples the input using 3x3 convolutions (stride=2, padding=1) and 1x1 convolutions (stride=2, no padding), adds them and applies ReLU activation on their sum.\n",
        "\n",
        "The rest of the layers use 3x3 conv (stride=1, padding=1), 1x1 conv (stride=1, no padding). Then the output of these layers is added along with an identity, and ReLU activation is applied.\n",
        "\n",
        "At the end of a stage, BatchNormalization is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGh7_jQ8QWU_"
      },
      "source": [
        "class stage(layers.Layer):\n",
        "    def __init__(self, filters, layer):\n",
        "        super(stage, self).__init__()\n",
        "        self.dow3 = layers.Conv2D(filters, kernel_size=3, strides=2, padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
        "        self.dow1 = layers.Conv2D(filters, kernel_size=1, strides=2, kernel_regularizer=regularizers.l2(1e-4))\n",
        "        self.con3 = layers.Conv2D(filters, kernel_size=3, padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
        "        self.con1 = layers.Conv2D(filters, kernel_size=1, padding='same', kernel_regularizer=regularizers.l2(1e-4))\n",
        "        self.bn = layers.BatchNormalization()\n",
        "        self.re = layers.Activation(activations.relu)\n",
        "        self.lay = layer\n",
        "\n",
        "    def call(self, inp):\n",
        "        x = self.dow3(inp)\n",
        "        y = self.dow1(inp)\n",
        "        x = x + y\n",
        "        x = self.re(x)\n",
        "        for i in range(self.lay-1):\n",
        "            y = self.con3(x)\n",
        "            z = self.con1(x)\n",
        "            x = x + y + z\n",
        "            x = self.re(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "    # def model(self):\n",
        "    #     x = layers.Input(shape=(224, 224, 3))\n",
        "    #     return tf.keras.Model(inputs=[x], outputs=self.call(x))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLecV01XIJHH"
      },
      "source": [
        "An implementation of RepVGG-A0 model. 5 stages are used with widths `[min(64, 64a), 64a, 128a, 256a, 512b]` and number of layers `[1, 2, 4, 14, 1]` respectively, and `a=0.75` and `b=2.5`. The 5 stages are then followed by GlobalAveragePooling layer and a fully connected layer with 10 channels (for CIFAR-10)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-xaOXuDY--k"
      },
      "source": [
        "class repvgg(tf.keras.Model):\n",
        "    def __init__(self, a, b, l=[1,2,4,14,1]):\n",
        "        super(repvgg, self).__init__()\n",
        "        self.st1 = stage(min(64, int(64*a)), 1)\n",
        "        self.st2 = stage(64*a, 2)\n",
        "        self.st3 = stage(128*a, 4)\n",
        "        self.st4 = stage(256*a, 14)\n",
        "        self.st5 = stage(512*b, 1)\n",
        "        self.gap = layers.GlobalAveragePooling2D()\n",
        "        self.end = layers.Dense(10, activation=\"softmax\", kernel_regularizer=regularizers.l2(1e-4))\n",
        "        \n",
        "\n",
        "    def call(self, inp):\n",
        "        x = self.st1(inp)\n",
        "        x = self.st2(x)\n",
        "        x = self.st3(x)\n",
        "        x = self.st4(x)\n",
        "        x = self.st5(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.end(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "    def model(self, train):\n",
        "        x = layers.Input(shape=train[0].shape)\n",
        "        return tf.keras.Model(inputs=x, outputs=self.call(x))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqZ1c2ryZVMN"
      },
      "source": [
        "model = repvgg(a=0.75, b=2.5)\n",
        "#print(model.model().summary())\n",
        "model.compile(  optimizer = tf.keras.optimizers.Adam(),#SGD(momentum=0.9, learning_rate=0.1),\n",
        "                loss = 'categorical_crossentropy',\n",
        "                metrics = ['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1ffr9yTDjd-"
      },
      "source": [
        "#For callbacks\n",
        "class mcb(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "       if(logs.get('accuracy')>0.95):\n",
        "            print(\"\\nReached 95% accuracy so cancelling training!\")\n",
        "            self.model.stop_training = True\n",
        "cb = mcb()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrSP4yeo5qkA",
        "outputId": "445f97aa-635c-4e14-898c-40430265c44e"
      },
      "source": [
        "model.fit(datagen.flow(trainIm, trainLab, batch_size=len(trainIm[0])), epochs=120, validation_data=(testIm, testLab), callbacks=[cb])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "1563/1563 [==============================] - 84s 51ms/step - loss: 2.1773 - accuracy: 0.2388 - val_loss: 2.1738 - val_accuracy: 0.2738\n",
            "Epoch 2/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 2.0616 - accuracy: 0.2774 - val_loss: 2.1260 - val_accuracy: 0.3037\n",
            "Epoch 3/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.9820 - accuracy: 0.3020 - val_loss: 2.0651 - val_accuracy: 0.2721\n",
            "Epoch 4/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.9304 - accuracy: 0.3115 - val_loss: 1.9819 - val_accuracy: 0.2829\n",
            "Epoch 5/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.8151 - accuracy: 0.3513 - val_loss: 1.9451 - val_accuracy: 0.2983\n",
            "Epoch 6/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.7695 - accuracy: 0.3656 - val_loss: 1.7004 - val_accuracy: 0.3301\n",
            "Epoch 7/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.7468 - accuracy: 0.3822 - val_loss: 1.8166 - val_accuracy: 0.3700\n",
            "Epoch 8/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6982 - accuracy: 0.4107 - val_loss: 17.2597 - val_accuracy: 0.2165\n",
            "Epoch 9/120\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 1.7094 - accuracy: 0.4226 - val_loss: 2.4748 - val_accuracy: 0.1429\n",
            "Epoch 10/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.8870 - accuracy: 0.3521 - val_loss: 1.8986 - val_accuracy: 0.3876\n",
            "Epoch 11/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.8147 - accuracy: 0.3611 - val_loss: 1.7047 - val_accuracy: 0.3652\n",
            "Epoch 12/120\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 1.6772 - accuracy: 0.4133 - val_loss: 1.8159 - val_accuracy: 0.3413\n",
            "Epoch 13/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6361 - accuracy: 0.4320 - val_loss: 3.3766 - val_accuracy: 0.1712\n",
            "Epoch 14/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6335 - accuracy: 0.4499 - val_loss: 1.8191 - val_accuracy: 0.3718\n",
            "Epoch 15/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6178 - accuracy: 0.4657 - val_loss: 2.5546 - val_accuracy: 0.2572\n",
            "Epoch 16/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6420 - accuracy: 0.4659 - val_loss: 1.8122 - val_accuracy: 0.3551\n",
            "Epoch 17/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6323 - accuracy: 0.4702 - val_loss: 1.6828 - val_accuracy: 0.4134\n",
            "Epoch 18/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6499 - accuracy: 0.4688 - val_loss: 1.6583 - val_accuracy: 0.4538\n",
            "Epoch 19/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.6509 - accuracy: 0.4717 - val_loss: 1.9923 - val_accuracy: 0.3287\n",
            "Epoch 20/120\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 1.7553 - accuracy: 0.4348 - val_loss: 2.3640 - val_accuracy: 0.2024\n",
            "Epoch 21/120\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 1.7673 - accuracy: 0.4304 - val_loss: 1.8177 - val_accuracy: 0.4141\n",
            "Epoch 22/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 1.7115 - accuracy: 0.4553 - val_loss: 1.8448 - val_accuracy: 0.4065\n",
            "Epoch 23/120\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 1.7665 - accuracy: 0.4387 - val_loss: 1.7771 - val_accuracy: 0.4475\n",
            "Epoch 24/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.7516 - accuracy: 0.4398 - val_loss: 2.1074 - val_accuracy: 0.2973\n",
            "Epoch 25/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.8093 - accuracy: 0.4209 - val_loss: 2.1797 - val_accuracy: 0.2900\n",
            "Epoch 26/120\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 1.8076 - accuracy: 0.4061 - val_loss: 1.7831 - val_accuracy: 0.4482\n",
            "Epoch 27/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.6332 - accuracy: 0.4698 - val_loss: 1.7075 - val_accuracy: 0.4303\n",
            "Epoch 28/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.4853 - accuracy: 0.5413 - val_loss: 1.8820 - val_accuracy: 0.4023\n",
            "Epoch 29/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 1.3445 - accuracy: 0.6026 - val_loss: 1.3295 - val_accuracy: 0.6053\n",
            "Epoch 30/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 1.2703 - accuracy: 0.6357 - val_loss: 1.3079 - val_accuracy: 0.6219\n",
            "Epoch 31/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 1.2185 - accuracy: 0.6551 - val_loss: 1.3576 - val_accuracy: 0.6065\n",
            "Epoch 32/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.1893 - accuracy: 0.6705 - val_loss: 1.2811 - val_accuracy: 0.6441\n",
            "Epoch 33/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.1667 - accuracy: 0.6830 - val_loss: 1.2583 - val_accuracy: 0.6468\n",
            "Epoch 34/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.1331 - accuracy: 0.6952 - val_loss: 1.2626 - val_accuracy: 0.6609\n",
            "Epoch 35/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 1.1234 - accuracy: 0.7037 - val_loss: 1.5646 - val_accuracy: 0.5851\n",
            "Epoch 36/120\n",
            "1563/1563 [==============================] - 79s 51ms/step - loss: 1.0553 - accuracy: 0.7263 - val_loss: 1.2176 - val_accuracy: 0.6741\n",
            "Epoch 37/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 1.0115 - accuracy: 0.7410 - val_loss: 1.0876 - val_accuracy: 0.7147\n",
            "Epoch 38/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.9823 - accuracy: 0.7513 - val_loss: 1.0830 - val_accuracy: 0.7164\n",
            "Epoch 39/120\n",
            "1563/1563 [==============================] - 82s 53ms/step - loss: 0.9579 - accuracy: 0.7601 - val_loss: 1.0982 - val_accuracy: 0.7175\n",
            "Epoch 40/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.9460 - accuracy: 0.7666 - val_loss: 1.0230 - val_accuracy: 0.7382\n",
            "Epoch 41/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.9159 - accuracy: 0.7737 - val_loss: 1.0526 - val_accuracy: 0.7231\n",
            "Epoch 42/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.8917 - accuracy: 0.7821 - val_loss: 1.0386 - val_accuracy: 0.7334\n",
            "Epoch 43/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.8793 - accuracy: 0.7880 - val_loss: 1.0383 - val_accuracy: 0.7368\n",
            "Epoch 44/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.8480 - accuracy: 0.7980 - val_loss: 1.0224 - val_accuracy: 0.7429\n",
            "Epoch 45/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.8396 - accuracy: 0.8019 - val_loss: 1.0583 - val_accuracy: 0.7268\n",
            "Epoch 46/120\n",
            "1563/1563 [==============================] - 82s 53ms/step - loss: 0.8244 - accuracy: 0.8074 - val_loss: 0.9958 - val_accuracy: 0.7510\n",
            "Epoch 47/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.8062 - accuracy: 0.8133 - val_loss: 1.0285 - val_accuracy: 0.7457\n",
            "Epoch 48/120\n",
            "1563/1563 [==============================] - 82s 53ms/step - loss: 0.8010 - accuracy: 0.8145 - val_loss: 1.0506 - val_accuracy: 0.7440\n",
            "Epoch 49/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.7891 - accuracy: 0.8220 - val_loss: 0.9645 - val_accuracy: 0.7621\n",
            "Epoch 50/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7784 - accuracy: 0.8251 - val_loss: 1.0382 - val_accuracy: 0.7447\n",
            "Epoch 51/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7670 - accuracy: 0.8289 - val_loss: 1.0357 - val_accuracy: 0.7571\n",
            "Epoch 52/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7605 - accuracy: 0.8319 - val_loss: 1.0025 - val_accuracy: 0.7552\n",
            "Epoch 53/120\n",
            "1563/1563 [==============================] - 82s 53ms/step - loss: 0.7541 - accuracy: 0.8356 - val_loss: 1.0397 - val_accuracy: 0.7571\n",
            "Epoch 54/120\n",
            "1563/1563 [==============================] - 82s 53ms/step - loss: 0.7494 - accuracy: 0.8376 - val_loss: 1.0666 - val_accuracy: 0.7465\n",
            "Epoch 55/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7465 - accuracy: 0.8370 - val_loss: 0.9994 - val_accuracy: 0.7603\n",
            "Epoch 56/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7418 - accuracy: 0.8403 - val_loss: 1.0343 - val_accuracy: 0.7587\n",
            "Epoch 57/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.7306 - accuracy: 0.8452 - val_loss: 1.0575 - val_accuracy: 0.7474\n",
            "Epoch 58/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7291 - accuracy: 0.8467 - val_loss: 0.9682 - val_accuracy: 0.7719\n",
            "Epoch 59/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7174 - accuracy: 0.8499 - val_loss: 1.0095 - val_accuracy: 0.7631\n",
            "Epoch 60/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7147 - accuracy: 0.8486 - val_loss: 1.0296 - val_accuracy: 0.7587\n",
            "Epoch 61/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.7108 - accuracy: 0.8541 - val_loss: 1.0192 - val_accuracy: 0.7572\n",
            "Epoch 62/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.7107 - accuracy: 0.8531 - val_loss: 1.0782 - val_accuracy: 0.7489\n",
            "Epoch 63/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.7055 - accuracy: 0.8571 - val_loss: 1.0390 - val_accuracy: 0.7621\n",
            "Epoch 64/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.7045 - accuracy: 0.8575 - val_loss: 0.9982 - val_accuracy: 0.7679\n",
            "Epoch 65/120\n",
            "1563/1563 [==============================] - 82s 53ms/step - loss: 0.6999 - accuracy: 0.8603 - val_loss: 1.0234 - val_accuracy: 0.7630\n",
            "Epoch 66/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6960 - accuracy: 0.8601 - val_loss: 1.0174 - val_accuracy: 0.7683\n",
            "Epoch 67/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6962 - accuracy: 0.8614 - val_loss: 1.0414 - val_accuracy: 0.7570\n",
            "Epoch 68/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6972 - accuracy: 0.8612 - val_loss: 1.0800 - val_accuracy: 0.7577\n",
            "Epoch 69/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6858 - accuracy: 0.8650 - val_loss: 1.0333 - val_accuracy: 0.7634\n",
            "Epoch 70/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6911 - accuracy: 0.8634 - val_loss: 1.0268 - val_accuracy: 0.7671\n",
            "Epoch 71/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6865 - accuracy: 0.8652 - val_loss: 1.0803 - val_accuracy: 0.7517\n",
            "Epoch 72/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6840 - accuracy: 0.8678 - val_loss: 1.0494 - val_accuracy: 0.7627\n",
            "Epoch 73/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6743 - accuracy: 0.8705 - val_loss: 1.0002 - val_accuracy: 0.7756\n",
            "Epoch 74/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6761 - accuracy: 0.8707 - val_loss: 1.0419 - val_accuracy: 0.7626\n",
            "Epoch 75/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6724 - accuracy: 0.8710 - val_loss: 1.0022 - val_accuracy: 0.7774\n",
            "Epoch 76/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6636 - accuracy: 0.8755 - val_loss: 1.0496 - val_accuracy: 0.7693\n",
            "Epoch 77/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6661 - accuracy: 0.8736 - val_loss: 1.0194 - val_accuracy: 0.7640\n",
            "Epoch 78/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6674 - accuracy: 0.8745 - val_loss: 1.0347 - val_accuracy: 0.7693\n",
            "Epoch 79/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6707 - accuracy: 0.8718 - val_loss: 1.0303 - val_accuracy: 0.7673\n",
            "Epoch 80/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6574 - accuracy: 0.8768 - val_loss: 1.0390 - val_accuracy: 0.7715\n",
            "Epoch 81/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6579 - accuracy: 0.8776 - val_loss: 1.0565 - val_accuracy: 0.7717\n",
            "Epoch 82/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6610 - accuracy: 0.8760 - val_loss: 1.0338 - val_accuracy: 0.7705\n",
            "Epoch 83/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6544 - accuracy: 0.8792 - val_loss: 0.9962 - val_accuracy: 0.7783\n",
            "Epoch 84/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6555 - accuracy: 0.8787 - val_loss: 1.0964 - val_accuracy: 0.7610\n",
            "Epoch 85/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6548 - accuracy: 0.8797 - val_loss: 1.0197 - val_accuracy: 0.7753\n",
            "Epoch 86/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6575 - accuracy: 0.8795 - val_loss: 1.0526 - val_accuracy: 0.7706\n",
            "Epoch 87/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6474 - accuracy: 0.8829 - val_loss: 1.0980 - val_accuracy: 0.7589\n",
            "Epoch 88/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6517 - accuracy: 0.8807 - val_loss: 1.0202 - val_accuracy: 0.7782\n",
            "Epoch 89/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6433 - accuracy: 0.8830 - val_loss: 1.0788 - val_accuracy: 0.7583\n",
            "Epoch 90/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6477 - accuracy: 0.8819 - val_loss: 1.0161 - val_accuracy: 0.7748\n",
            "Epoch 91/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.6368 - accuracy: 0.8862 - val_loss: 1.0774 - val_accuracy: 0.7636\n",
            "Epoch 92/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6491 - accuracy: 0.8841 - val_loss: 1.0788 - val_accuracy: 0.7621\n",
            "Epoch 93/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6453 - accuracy: 0.8832 - val_loss: 1.0795 - val_accuracy: 0.7660\n",
            "Epoch 94/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6476 - accuracy: 0.8836 - val_loss: 1.0435 - val_accuracy: 0.7734\n",
            "Epoch 95/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6423 - accuracy: 0.8862 - val_loss: 1.0414 - val_accuracy: 0.7769\n",
            "Epoch 96/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6324 - accuracy: 0.8879 - val_loss: 1.0257 - val_accuracy: 0.7778\n",
            "Epoch 97/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.6351 - accuracy: 0.8877 - val_loss: 1.0625 - val_accuracy: 0.7729\n",
            "Epoch 98/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6365 - accuracy: 0.8853 - val_loss: 1.0292 - val_accuracy: 0.7769\n",
            "Epoch 99/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6375 - accuracy: 0.8866 - val_loss: 1.1090 - val_accuracy: 0.7625\n",
            "Epoch 100/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6332 - accuracy: 0.8890 - val_loss: 1.0494 - val_accuracy: 0.7713\n",
            "Epoch 101/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6345 - accuracy: 0.8894 - val_loss: 1.0134 - val_accuracy: 0.7800\n",
            "Epoch 102/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6246 - accuracy: 0.8941 - val_loss: 1.1117 - val_accuracy: 0.7630\n",
            "Epoch 103/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6262 - accuracy: 0.8907 - val_loss: 1.0562 - val_accuracy: 0.7712\n",
            "Epoch 104/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6267 - accuracy: 0.8907 - val_loss: 1.0600 - val_accuracy: 0.7755\n",
            "Epoch 105/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6235 - accuracy: 0.8924 - val_loss: 1.0560 - val_accuracy: 0.7703\n",
            "Epoch 106/120\n",
            "1563/1563 [==============================] - 80s 51ms/step - loss: 0.6288 - accuracy: 0.8910 - val_loss: 1.0283 - val_accuracy: 0.7803\n",
            "Epoch 107/120\n",
            "1563/1563 [==============================] - 81s 52ms/step - loss: 0.6213 - accuracy: 0.8936 - val_loss: 1.0303 - val_accuracy: 0.7829\n",
            "Epoch 108/120\n",
            "1563/1563 [==============================] - 82s 52ms/step - loss: 0.6235 - accuracy: 0.8924 - val_loss: 1.1120 - val_accuracy: 0.7572\n",
            "Epoch 109/120\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.6247 - accuracy: 0.8926 - val_loss: 1.0637 - val_accuracy: 0.7704\n",
            "Epoch 110/120\n",
            "1563/1563 [==============================] - 84s 54ms/step - loss: 0.6219 - accuracy: 0.8934 - val_loss: 1.0782 - val_accuracy: 0.7749\n",
            "Epoch 111/120\n",
            "1563/1563 [==============================] - 85s 55ms/step - loss: 0.6197 - accuracy: 0.8934 - val_loss: 1.0452 - val_accuracy: 0.7782\n",
            "Epoch 112/120\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.6171 - accuracy: 0.8954 - val_loss: 1.0779 - val_accuracy: 0.7756\n",
            "Epoch 113/120\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.6200 - accuracy: 0.8943 - val_loss: 1.0647 - val_accuracy: 0.7725\n",
            "Epoch 114/120\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.6189 - accuracy: 0.8950 - val_loss: 1.0677 - val_accuracy: 0.7727\n",
            "Epoch 115/120\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.6109 - accuracy: 0.8981 - val_loss: 1.0810 - val_accuracy: 0.7768\n",
            "Epoch 116/120\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.6216 - accuracy: 0.8940 - val_loss: 1.0985 - val_accuracy: 0.7708\n",
            "Epoch 117/120\n",
            "1563/1563 [==============================] - 84s 53ms/step - loss: 0.6163 - accuracy: 0.8956 - val_loss: 1.0754 - val_accuracy: 0.7783\n",
            "Epoch 118/120\n",
            "1563/1563 [==============================] - 84s 53ms/step - loss: 0.6125 - accuracy: 0.8969 - val_loss: 1.0270 - val_accuracy: 0.7835\n",
            "Epoch 119/120\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.6164 - accuracy: 0.8969 - val_loss: 1.0878 - val_accuracy: 0.7683\n",
            "Epoch 120/120\n",
            "1563/1563 [==============================] - 83s 53ms/step - loss: 0.6172 - accuracy: 0.8981 - val_loss: 1.0403 - val_accuracy: 0.7803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3920145d90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ultWGPC9LBIb"
      },
      "source": [
        "Achieves an accuracy of **78.03%** on CIFAR-10 test dataset, with 89.81% accuracy on training dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVWZvNu7HAEf",
        "outputId": "41f85ecf-c116-4ef0-d436-76836f050fa4"
      },
      "source": [
        "model.evaluate(testIm, testLab)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 5s 15ms/step - loss: 1.0403 - accuracy: 0.7803\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0403088331222534, 0.7803000211715698]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmnB2bqzHMnq",
        "outputId": "9f1a1e42-ed24-41f1-9697-15e8dfb0882c"
      },
      "source": [
        "print(model.model(trainIm).summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "stage (stage)                (None, 16, 16, 48)        1728      \n",
            "_________________________________________________________________\n",
            "stage_1 (stage)              (None, 8, 8, 48)          46464     \n",
            "_________________________________________________________________\n",
            "stage_2 (stage)              (None, 4, 4, 96)          139008    \n",
            "_________________________________________________________________\n",
            "stage_3 (stage)              (None, 2, 2, 192)         554496    \n",
            "_________________________________________________________________\n",
            "stage_4 (stage)              (None, 1, 1, 1280)        2465280   \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1280)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                12810     \n",
            "=================================================================\n",
            "Total params: 3,219,786\n",
            "Trainable params: 3,216,458\n",
            "Non-trainable params: 3,328\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}